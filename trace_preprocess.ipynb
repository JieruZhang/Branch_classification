{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM-branch predictor数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把raw的trace文件转换成句子文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace2sentence(filename, window_length):\n",
    "    trace_file = open(\"D:/tensorflow/Branch_Prediction/RNN_Branch_prediction/data/trace/trace_file.txt\", 'w')\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        for num in range(len(lines)-window_length+1):\n",
    "            for items in lines[num:num+window_length]:\n",
    "                words = items.strip().split(',')\n",
    "                word = words[0] + words[1]\n",
    "                trace_file.write(word + ' ')\n",
    "            trace_file.write('\\n')\n",
    "    trace_file.close()\n",
    "    \n",
    "trace2sentence('SHORT_SERVER-103.bt9.trace', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filename, percent):\n",
    "    train_file = open(\"D:/tensorflow/Branch_Prediction/RNN_Branch_prediction/data/trace/train.txt\",'w')\n",
    "    dev_file = open(\"D:/tensorflow/Branch_Prediction/RNN_Branch_prediction/data/trace/dev.txt\", 'w')\n",
    "    test_file = open(\"D:/tensorflow/Branch_Prediction/RNN_Branch_prediction/data/trace/test.txt\", 'w')\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        num_lines = int(percent * len(lines))\n",
    "        train_num = int(0.8 * num_lines)\n",
    "        dev_num = int(0.1 * num_lines)\n",
    "        test_num = num_lines -train_num - dev_num\n",
    "        for line in lines[0:train_num+1]:\n",
    "            train_file.write(line)\n",
    "        for line in lines[train_num+1: train_num + dev_num + 1]:\n",
    "            dev_file.write(line)\n",
    "        for line in lines[train_num + dev_num + 1: num_lines + 1]:\n",
    "            test_file.write(line)\n",
    "    train_file.close()\n",
    "    dev_file.close()\n",
    "    test_file.close()\n",
    "    \n",
    "split_dataset(\"trace_file.txt\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把raw的trace文件转换成句子文件，并添加标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace2sentenceLabel(filename, window_length):\n",
    "    trace_file = open(\"E:/traces/SHORT_SERVER-101/20/trace_file.txt\", 'w')\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        for num in range(len(lines)-window_length+1):\n",
    "            for items in lines[num:num+(window_length-1)]:\n",
    "                words = items.strip().split(',')\n",
    "                word = words[0] + words[1]\n",
    "                trace_file.write(word + ' ')\n",
    "            words = lines[num+window_length-1].strip().split(',')\n",
    "            trace_file.write(words[0] + '\\t' + words[1])\n",
    "            trace_file.write('\\n')\n",
    "    trace_file.close()\n",
    "    \n",
    "trace2sentenceLabel('SHORT_SERVER-101.bt9.trace', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset2(filename, percent):\n",
    "    train_file = open(\"E:/traces/SHORT_SERVER-101/20/trace_train.txt\",'w')\n",
    "    dev_file = open(\"E:/traces/SHORT_SERVER-101/20/trace_val.txt\", 'w')\n",
    "    test_file = open(\"E:/traces/SHORT_SERVER-101/20/trace_test.txt\", 'w')\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        num_lines = int(percent * len(lines))\n",
    "        train_num = int(0.8 * num_lines)\n",
    "        dev_num = int(0.1 * num_lines)\n",
    "        test_num = num_lines -train_num - dev_num\n",
    "        for line in lines[0:train_num+1]:\n",
    "            train_file.write(line)\n",
    "        for line in lines[train_num+1: train_num + dev_num + 1]:\n",
    "            dev_file.write(line)\n",
    "        for line in lines[train_num + dev_num + 1: num_lines + 1]:\n",
    "            test_file.write(line)\n",
    "    train_file.close()\n",
    "    dev_file.close()\n",
    "    test_file.close()\n",
    "    \n",
    "split_dataset2(\"E:/traces/SHORT_SERVER-101/20/trace_file.txt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  6  9]\n",
      " [ 0  3  6  1  5]\n",
      " [ 3  6  1  0  7]\n",
      " [ 6  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  6  9]\n",
      " [ 0  4  6 10  5]\n",
      " [ 4  6 10  0  7]\n",
      " [ 6 10  0  3  8]\n",
      " [10  0  3  6  9]\n",
      " [ 0  3  6  1  5]\n",
      " [ 3  6  1  0  7]\n",
      " [ 6  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]]\n",
      "[1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0]\n",
      "[[ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  6  9]\n",
      " [ 0  3  6  1  5]\n",
      " [ 3  6  1  0  7]\n",
      " [ 6  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  6  9]\n",
      " [ 0  4  6 10  5]\n",
      " [ 4  6 10  0  7]\n",
      " [ 6 10  0  3  8]\n",
      " [10  0  3  6  9]\n",
      " [ 0  3  6  1  5]\n",
      " [ 3  6  1  0  7]\n",
      " [ 6  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]\n",
      " [ 2  1  0  4  8]\n",
      " [ 1  0  4  2  9]\n",
      " [ 0  4  2  1  5]\n",
      " [ 4  2  1  0  7]\n",
      " [ 2  1  0  3  8]\n",
      " [ 1  0  3  2  9]\n",
      " [ 0  3  2  1  5]\n",
      " [ 3  2  1  0  7]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据, contents = [[],[]...]\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            content, label = line.strip().split('\\t')\n",
    "            contents.append(content.strip().split())\n",
    "            labels.append(label)\n",
    "    return contents, labels\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "    data = []\n",
    "    for item in data_train:\n",
    "        #data = data+item   \n",
    "        data.extend(item)\n",
    "    counter = Counter(data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    open(vocab_dir, 'w').write('\\n'.join(words) + '\\n')\n",
    "    \n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    with open(vocab_dir, 'r') as fp:\n",
    "        words = [_.strip() for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['0', '1']\n",
    "    categories = [x for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id,max_length = 5):\n",
    "    \"\"\"将文件转换为id表示, data_id = [[],[]...]\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "    #return data_id, label_id\n",
    "    data_id = np.array(data_id)\n",
    "    label_id = np.array(label_id)\n",
    "    print(data_id)\n",
    "    print(label_id)\n",
    "    return x_pad, y_pad\n",
    "\n",
    "_, cat_to_id = read_category()\n",
    "_, word_to_id = read_vocab('vocab.txt')\n",
    "a, b = process_file('trace_file.txt', word_to_id, cat_to_id)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1]]\n",
    "a = np.array(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
